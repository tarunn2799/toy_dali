{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.ops as ops\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator, DALIClassificationIterator\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import nvidia.dali.fn as fn\n",
    "import cupy\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Source Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassIterator(object):\n",
    "    def __init__(self, datasets, batch_size):\n",
    "        len_df = [len(df) for df in datasets]\n",
    "        self.largest_dataset = max(len_df) #length of csv file with the most number of datapoints\n",
    "        self.largest_dataset_idx = len_df.index(self.largest_dataset) #identifier for which one the largest is\n",
    "        self.counter_index = {i: 0 for i, dataset in enumerate(datasets)} #a counter to see how many datapoints have been taken from each dataset\n",
    "        self.counter_dataset = 0 #identifier for which dataset we're currently on\n",
    "        self.iterable_datasets = itertools.cycle(datasets)\n",
    "        self.batch_size = batch_size\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def customroundrobin(self, iterable, index): \n",
    "        '''\n",
    "        Performs a round robin over the smaller dataset. In the case where one CSV is smaller than the other, the\n",
    "        smaller one is iterated through again till we reach the length of the largest CSV dataset.\n",
    "        '''\n",
    "        start_over = 0\n",
    "        if index >= len(iterable):\n",
    "            start_over += 1\n",
    "        while True:\n",
    "            for i, element in enumerate(iterable):\n",
    "                if i >= index or start_over:\n",
    "                    if i == len(iterable) - 1:\n",
    "                        start_over += 1\n",
    "                    yield element\n",
    "                    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        batch_crops = []\n",
    "        batch_labels = [np.array(i) for i in range(4)] #Ignore this, just a dummy label being generated. \n",
    "        print('Counter Index Updated', self.counter_index)\n",
    "        \n",
    "        #takes a row from our CSV file dataset, and appends the result values to a list. Value is yielded.\n",
    "        batch_counter = 0\n",
    "        cur_iterable = self.customroundrobin(self.iterable_datasets.__next__(), self.counter_index[self.counter_dataset])\n",
    "        while batch_counter < self.batch_size:\n",
    "            self.counter_index[self.counter_dataset] += 1\n",
    "            data_point = cur_iterable.__next__()\n",
    "            crops = data_point[\"labels_crops\"]\n",
    "            crops = ast.literal_eval(crops)\n",
    "            crops = [crops[0], crops[1], crops[2] - crops[0], crops[3] - crops[1]] #Format to support ops.slice\n",
    "            batch_crops.append(np.array([crops], dtype=np.int32))\n",
    "            batch_counter += 1\n",
    "        self.counter_dataset += 1\n",
    "        if self.counter_dataset == len(datasets):\n",
    "            self.counter_dataset = 0\n",
    "        yield (batch_crops, batch_labels)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.largest_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericIterator(DALIGenericIterator):\n",
    "    def __init__(self, **args):\n",
    "        super().__init__(**args)\n",
    "        pass\n",
    "    \n",
    "    def custom_collate(self, loader_dict):\n",
    "        #using this function for adding custom collate functionalit\n",
    "        pass\n",
    "\n",
    "    def __next__(self):\n",
    "        #Removed a few nuances, but we do require a custom DALIGenericIterator (for custom functions)\n",
    "        loader_dict = {}\n",
    "        out = super().__next__()\n",
    "        out = out[0]\n",
    "        loader_dict[\"input\"] = out[self.output_map[0]].float()\n",
    "        loader_dict[\"labels\"] = torch.squeeze(out[self.output_map[1]])\n",
    "\n",
    "        return loader_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalSourcePipeline(Pipeline):\n",
    "    def __init__(self, file_list, batch_size, num_threads, device_id, external_data):\n",
    "        super(ExternalSourcePipeline, self).__init__(batch_size, num_threads, device_id)\n",
    "        self.input = ops.FileReader(file_list= file_list)\n",
    "        self.label = ops.ExternalSource()\n",
    "        self.crops = ops.ExternalSource()\n",
    "        self.decode = ops.ImageDecoder(device=\"mixed\", output_type=types.RGB)\n",
    "        self.res = ops.Resize(device=\"gpu\", resize_x=224, resize_y=224)\n",
    "        self.cast = ops.Cast(device=\"cpu\", dtype=types.INT32)\n",
    "        self.external_data = external_data\n",
    "        self.iterator = iter(self.external_data)\n",
    "        \n",
    "    def define_graph(self):\n",
    "        jpegs, dummy_labels = self.input()\n",
    "        self.labels = self.label()\n",
    "        self.crop_dim = self.crops()\n",
    "        anchor =  fn.reshape(fn.slice(self.crop_dim, 0, 2, axes=[1]), shape=[-1])\n",
    "        shape = fn.reshape(fn.slice(self.crop_dim, 2, 2, axes = [1]), shape= [-1])\n",
    "        anchor = self.cast(anchor)\n",
    "        shape = self.cast(shape)\n",
    "        images = self.decode(jpegs)\n",
    "        images = self.res(images)\n",
    "\n",
    "\n",
    "#       decode and slicing\n",
    "        jpegs = fn.slice(jpegs, anchor, shape, axes= [0,1], device= 'gpu')\n",
    "        jpegs = self.res(jpegs)\n",
    "\n",
    "        return (images, self.labels, self.crop_dim)\n",
    "\n",
    "    def iter_setup(self):\n",
    "            print('Entering iter_setup func')\n",
    "            crops, labels = list(next(self.iterator))[0]\n",
    "            print(crops, labels)\n",
    "            self.feed_input(self.labels, labels)\n",
    "            self.feed_input(self.crop_dim, crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [pd.read_csv(df, index_col= 'Unnamed: 0').to_dict(orient='records') for df in glob.glob('*.csv')]\n",
    "\n",
    "\n",
    "multi_iter = MultiClassIterator(datasets, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExternalSourcePipeline(file_list = 'single_image.txt' ,batch_size= 4, num_threads=2, device_id=0,\n",
    "                                  external_data=multi_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering iter_setup func\n",
      "Counter Index Updated {0: 0, 1: 0}\n",
      "[array([[ 177,  612, 1713, 1920]], dtype=int32), array([[ 177,  612, 1713, 1920]], dtype=int32), array([[ 339,  768, 1389, 1587]], dtype=int32), array([[ 339,  768, 1389, 1587]], dtype=int32)] [array(0), array(1), array(2), array(3)]\n",
      "Entering iter_setup func\n",
      "Counter Index Updated {0: 4, 1: 0}\n",
      "[array([[158, 292,  96, 183]], dtype=int32), array([[137, 252, 147, 348]], dtype=int32), array([[406, 638, 283, 795]], dtype=int32), array([[128, 214, 132, 386]], dtype=int32)] [array(0), array(1), array(2), array(3)]\n",
      "Entering iter_setup func\n",
      "Counter Index Updated {0: 4, 1: 4}\n",
      "[array([[ 494,  834, 1042, 1388]], dtype=int32), array([[ 494,  834, 1042, 1388]], dtype=int32), array([[642, 649, 798, 761]], dtype=int32), array([[ 413,  915, 1278, 1174]], dtype=int32)] [array(0), array(1), array(2), array(3)]\n"
     ]
    }
   ],
   "source": [
    "pii = GenericIterator(pipe, output_map=['data', 'label', 'crops'], auto_reset = True, size = multi_iter.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}